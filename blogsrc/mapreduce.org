#+TITLE: MapReduce 使用经验
#+SETUPFILE: ../setup/theme-readtheorg-local.setup
#+OPTIONS: toc:nil

* 陷阱
** Mutability
所有 *Writable* 类型都是可变的，例如Text，LongWritable等，不要保持对他们的引用，甚至 =for (Text value : values)= 都不能保持对value的引用，因为它们都指向同一个对象。

看下面的代码，values是reduce的第二个参数，这样写正确吗？

#+BEGIN_SRC java
Text max = null;
for (Text value : values) {
  if (max == null) max = value;
  else if (max.getLength() < value.getLength()) max = value;
}
#+END_SRC

** 运行上下文
我非常好奇为什么 *Configuration* 不能设置value是 *Object* 呢？因为我想在 *main* 里初始化一个对象，然后在map和reduce里面使用，后来发现不支持，于是我把它做成一个单例。在 *local* 模式下竟然可以运行，可是在 *distributed* 模式下不行，为什么？

因为 *local* 模式， *main* *map* *reduce* 全在一个进程中运行， *main* 中初始化的单例对象， *map* 和 *reduce* 中可以使用。而在 *distributed* 模式下，这三者在不同机器的不同进程中运行，它们逻辑上是一个程序，但运行上是三个程序。只可以在 *main* 中设置 *Configuration* ， *map 和 *reduce* 中读取里面的值。

** LOGGER
在mapreduce程序中可以像通常的java程序一样使用 *slf4j* 等工具打印日志，但是在具体的 *Mapper* 和 *Reducer* 中不建议打印日志（含System.out），巨慢无比，可以使用 *Counter* 等排查工具，或者把文件复制到本地，用local模式运行。另外查看 *Mapper* 和 *Reduce* 中的日志，无法在标准输出中看到，应该在任务控制台看。

** 强制结束mapreduce程序
在命令行用 *Ctrl-C* 或者 *kill* 没办法真的停止mapreduce程序，必须使用专门的命令 =yarn application -kill <application-id>=

** 使用 hadoop fs -copyFromLocal 复制文件
如果并发的向hadoop复制同一个文件，导致错误。 =hdfs.DFSClient: DataStreamer Exception org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/test/2017-12-11-09_log._COPYING_ (inode 252293331): File does not exist=

* 技巧
** 在Java7平台运行Java8应用
当Hadoop、yarn和应用使用的Java版本不同时，需要配置应用自己的 =JAVA_HOME= （联想MapReduce的工作流程，想想为什么这样是可行的）， ~-D yarn.app.mapreduce.am.env=JAVA_HOME=/path_to_java8 -D mapreduce.admin.user.env=JAVA_HOME=/path_to_java8~

** *local* 配置优化
*** 加大java内存配置
修改 =hadoop-env.sh= 的 ~HADOOP_CLIENT_OPTS~ 例如：~export HADOOP_CLIENT_OPTS="-Xmx1024m $HADOOP_CLIENT_OPTS"~

*** 修改tmp目录到大磁盘
修改 =mapred-site.xml= 的 =mapreduce.cluster.local.dir=

* avro
=avro= 是类似于 =protobuf= 的序列化机制，支持在mapreduce中使用。除了有schema外，avro降低了文件的大小。

** *local* 模式注意事项
hadoop内置了avro的lib，这个可能和应用使用的avro版本不一致，在 *local* 模式下需要配置两个环境变量。
#+BEGIN_SRC bash
export HADOOP_CLASSPATH=/path_to_you_jar
export HADOOP_USER_CLASSPATH_FIRST=true
#+END_SRC

** 修改模式
新增字段时，为了兼容老数据，需要指定 =default= ，如果 =type= 是多个类型，则 =default= 类型必须和第一个相同，例如：
#+BEGIN_EXAMPLE
{"name": "uid", "type": ["null", "string"], "default": null}
#+END_EXAMPLE
