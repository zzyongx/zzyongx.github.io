#+TITLE: 当我们谈论分布式系统时，我们谈论什么
#+SETUPFILE: ../setup/theme-readtheorg-local.setup
#+OPTIONS: toc:nil

当我们面对可用性，可扩展性，海量吞吐等问题时，总是免不了提起分布式系统。无论是自研还是开源，当我们面对分布式系统时，我们需要关注什么？如何对比两个系统呢？

* 无状态系统
所谓无状态系统，节点间不共享数据，节点通过负载均衡或者 *Keepalive* 维持松散的关系，节点通常是对称的。例如：nginx集群。这类系统的搭建和维护非常容易，以下主要讨论有状态的系统。

* 负载均衡
** 均衡请求
分布式系统通常有很多节点，到底该请求哪个节点？如果正在服务的节点挂了怎么办？一种方法是，节点提供VIP，客户端访问VIP，节点不可用时，VIP自动漂到别的节点，但维护VIP很复杂，这种方式并不多见。另一种方式是，客户端有完整的服务节点，随机挑选一个使用，当节点不可用时，切换到其它节点。多数系统，节点是不对称，此时有两种应对方法，一种是节点告知客户端该请求哪个节点，客户端把请求发到正确的节点。例如：hadoop的namenode做HA时，如果请求了standby节点，会返回错误，此时需要重试另外的节点。另一种方法是，节点作为代理，把请求发给正确的节点。例如：zookeeper，leader节点处理写请求，如果写请求到了follower节点，follower节点把写请求转发给leader节点。

** 均衡数据
常见的有两种，基于哈希（或一致性哈希）或者元数据。例如：Cassandra根据哈希把数据分到vnode节点，hadoop利用NameNode决定数据该分到哪个DataNode。NameNode方案无疑更好，因为DataNode是对等的，任何DataNode节点都可以接受任何数据，NameNode负责登记数据项和数据位置的关系，但NameNode容量有限，并且每次访问都要访问NameNode。而Cassandra的节点不对等，每个节点只负责指定哈希的数据。

除了考虑数据分布均衡，还要考虑新旧数据是否均衡，尤其是基于NameNode的均衡策略。通常而言，新数据的访问量比旧数据大，数据新旧分布不均会导致请求负载不均衡。
Kafka采用Zookeeper存放topic的位置，但是topic的数据量和请求量都是无法实现确认的，也非常容易不均衡。

*** hadoop如何应对新增节点，更换磁盘
NameNode默认用round-roubin把数据均衡的分布在DataNode上，而不考虑DataNode的容量，以免新数据过多的放到新节点，发生数据新旧不均衡。当更换磁盘时，DataNode也不考虑磁盘容量，以免新数据会大量写在新磁盘，导致新旧数据磁盘间不均衡。所以需要经常rebalance，以免容量不均衡。

** 系统是否均衡
通常新增节点，或者更换磁盘，系统肯定是不均衡的，此时需要rebalance，以保持数据分布均衡。但是rebalance需要额外消耗集群资源，尤其是当集群资源不足新增节点时，节点新增的服务能力可能不抵rebalance带来的资源消耗，导致集群资源更加不足。通常要手动触发rebalance。如果设计自动rebalance的系统，需要有机制可以终止，以免rebalance无法不可控。

除此外，系统分布数据的策略，客户端选择节点的策略也影响均衡。

* 异构系统支持
硬件有换代周期，新旧硬件共存的情况非常普遍，对支持异构系统的支持非常重要。Cassandra支持节点配置不同的vnode，Yarn支持配置vcore和系统可用内存。

* CAP 理论
简单说，CAP是在一致性（CP）和可用性（AP）之间做选择（P没得选）。可是什么是可用性，什么是一致性呢？如果把可用性定义为，系统总是可以读写，那么写入系统的数据一段时间内读不出来咋办，甚至丢失咋办，这种可用性有啥意义？例如，订单系统采用一主多从的MySQL高可用方案，如果总是读从库，可能出现下单了，但是查不到的情况。核心是根据分布式系统的特点，设计业务逻辑。

** 什么是一致性
如果定义一致性为，副本间的数据和最新写入的数据保持一致。那如何判定最新？在分布式系统中，绝对时间是不存在的。为了定义最新，需要一个单调递增的序列生成器。而高性能，高可用的序列生成器本身就是一个复杂的分布式系统。折衷的办法是，用时间戳判断新旧。

从客户端角度看，一致性有三种情况，假设A，B是两个客户端。

*** 强一致性
A写入，A，B都可以读到A写入的数据。似乎只有Hadoop做到了这点。这是由hadoop的业务特点决定的，写入量并不大，几乎没有更新操作。

*** 因果一致性
A写入，A可以读到A写入的数据，B不确定。Zookeeper应该做到了这一点。ZK并不是强一致的系统，ZK有三种角色，Leader、Follower和Observer，Leader和Follower构成quorum后，就提交事务，此时更新并没有扩散到所有节点。

*** 最终一致性
最终一致性的含义是A写入，A，B最终都会读到A写入的数据，但是什么时候读到，不确定。极端情况下，永远也读不到也是有可能的。例如三副本系统，写入一个副本就算成功，最终更新会复制到其它两个副本，如果再更新扩散到其它两个副本前磁盘坏掉了，数据可能就丢失。如果WAL放在单独的磁盘，并且数据扩散到足够的副本前，WAL没有删除，则可以从WAL中恢复。

最终一致性有个变形，可调一致性。可以每次写入配置写入副本的数量。这在一定程度上提高了数据的持久性和一致性。例如三副本系统，每次写两个副本，读两个副本，挑选 *最新* 的数据返回。这降低了系统的性能，并且可能影响可用性，取决于没有副本数量不足时的策略。Cassandra，kafka都有可配一致性的概念。

* 副本放置策略
** 机架、机房属性
支持机架，机房属性是为了可用性，机架属性比较简单，同机房不同机架网络延迟可以忽略，不同机房延迟则较大。分布式系统通常对节点间网络延迟有个预估，根据这个来设置各种超时。


* 多租户的支持
多个用户同时使用系统时，如何保证用户数据的安全性和可用性。如果对多租户支持不好，那么要为多个用户搭建不同的集群，资源利用率和维护成本和用户数量线性相关。

** 认证
基于用户名和密码访问。不同的用户，权限相同。例如：redis支持设置用户名、密码。

** 授权
资源区分类型和命名空间，不同用户权限不同。例如：Cassandra 不同的用户访问不同的库。

** 资源隔离
对于给不同用户提供服务的分布式系统，确保单个用户的访问不会拖死系统尤其重要。如果系统提供的接口资源消耗差别巨大，不同用户最好不要共享实例。例如：MySQL的不同查询资源消耗差别巨大。但是有些系统自带了资源隔离机制，例如：MapReduce可以配置用户cpu和内存的使用，实现不同用户间资源隔离。纵然如此，单个用户耗尽没有隔离的资源也是有可能的。例如，MapReduce任务产生过量的临时磁盘IO。

* 使用开源软件的正确姿势
** 社区活跃度
社区活跃度高，说明用户很多，软件经过了生产环境的检验，软件的问题能够及时被发现和修复。同时碰到问题也能及时从社区获得帮助。

** 关注软件最新进展
如果在使用过程中碰到问题，可以从issue或者google寻找答案，如果不是软件bug，而是配置和用法的问题，通常比较好解决。查看软件的ChangeLog，看新版中是否修复了此类问题。在issue中向开发者提问也是一个好办法。

另外就算没有碰到问题，也要关注软件的最新进展，最好周期性的升级（以半年为单位）。升级当然带来了不确定性，但是好处更多。首先可以使用新版的新特性，其次新版的社区活跃度更高，再者长期不升级可能就没法升级了。例如：Java7升级8可能比较简单，但是Java6升级8则没那么容易。

** 测试开源软件
不存在万金油软件，了解软件的使用场景和业务是否匹配，有时还需要在业务需求和软件能力间做折衷。例如很多场景丢数据是绝对无法忍受的，有些场景则可以丢数据，但是不允许没有响应。

分析软件高性能，高可用，可扩展性，数据持久性，数据一致性机制。尝试从配置，API，管理接口（可能是命令、接口、JMX等，接口暴露了系统的内部状态），文档入手了解这些机制。建立针对这些机制的监控，并模拟磁盘、机器故障，网络不稳定，增删机器等，看这些机制是否工作（检验配置是否正确，报警是否工作，是否能够正确应对这些故障）。

了解软件的陷阱，建立一套最佳实践。很多问题都是使用不当造成的。例如：hdfs的小文件问题，mapreduce的数据不可split等。
